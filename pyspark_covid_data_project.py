# -*- coding: utf-8 -*-
"""PySpark_Covid_Data_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1raDlliPr5_WOGyinzorurQrZl_noWknx
"""

from google.colab import files
uploaded = files.upload()

"""# 1. Data Loading"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('Covid Data Analysis').getOrCreate()

spark

df = spark.read.csv('covid-data.csv', header = True, inferSchema = True)

"""# 2. Data Cleaning"""

from pyspark.sql.functions import to_date

df = df.withColumn('date', to_date('date', 'yyyy-MM-dd')).dropna(subset = ['location', 'date', 'total_cases'])

df.show()

"""# 3. Total Cases and Deaths by Country"""

from pyspark.sql.functions import max

cases_deaths = df.groupBy('location').agg(max('total_cases').alias('Total_Cases'), max('total_deaths').alias('Total_Deaths')).orderBy('Total_Cases', ascending = False)

cases_deaths.show()

"""# 4. Daily new cases trend for a country"""

india_trend = df.filter(df['location'] == 'India').select('date', 'new_cases').orderBy('date')

india_trend.show()

"""# 5. Vaccination progress by country"""

vaccination = df.groupBy('location').agg(max('people_vaccinated').alias('People_Vaccinated'), max('total_vaccinations').alias('Total_Vaccinations')).orderBy('People_Vaccinated', ascending = False)

vaccination.show()

"""# 6. Mortality Rate by Country"""

from pyspark.sql.functions import col, round

mortality = df.groupBy('location').agg(max('total_cases').alias('cases'), max('total_deaths').alias('deaths'))\
.withColumn('mortality_rate', round((col('deaths') / col('cases'))*100, 2))\
.orderBy('mortality_rate', ascending = False)

mortality.show()

"""# 7. Identify Countries with late Vaccine Rollout"""

from pyspark.sql.functions import min, when

df_vaccine = df.filter(col('people_vaccinated') > 0).groupBy('location').agg(min('date').alias('vaccine_start_date'))

df_vaccine.orderBy('vaccine_start_date').show()

"""# 8. Compare first and second wave for selected country"""

from pyspark.sql.functions import sum
first_wave = df.filter((col('location') == 'India') & (col('date') < '2021-01-01'))
second_wave = df.filter((col('location') == 'India') & (col('date') >= '2021-01-01') & (col('date') < '2021-06-01'))


first_wave_total = first_wave.agg(sum('new_cases').alias("First_wave_cases"))
second_wave_total = second_wave.agg(sum('new_cases').alias('Second_wave_cases'))

# Show results
first_wave_total.show()
second_wave_total.show()

"""# 9. Top Countries by testing rate (Tests per thousand people)"""

df_tests = df.groupBy('location').agg(max('total_tests_per_thousand').alias('max_test_rate')).orderBy(col('max_test_rate').desc())

df_tests.show()

"""So, 32,925.826 tests per 1,000 people means Cyprus conducted the equivalent of ~32 full tests per person, possibly because:

People were tested multiple times (e.g., for travel, work, or symptoms).

Small countries with frequent and aggressive testing tend to show very high numbers.

Cumulative metric: This number grows over time as testing increases.

# 10. Countries with most fluctuating new cases (Volatility)
"""

from pyspark.sql.functions import stddev
df_fluctuation = df.groupBy('location').agg(stddev('new_cases').alias('std_cases')).orderBy(col('std_cases').desc())

df_fluctuation.show(5)

#spark.stop()

